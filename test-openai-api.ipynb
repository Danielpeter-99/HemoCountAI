{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name: str = \"gpt-4\"\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base: str = \"https://hemocountopenai-westus.openai.azure.com/\"\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version: str = \"2023-03-15-preview\"\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m---> 12\u001b[0m current_script_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# ### Setup Parameters\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Load config values\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path(current_script_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mopen() \u001b[38;5;28;01mas\u001b[39;00m config_file:\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# <h1 align =\"center\"> Shared Functions</h1>\n",
    "# <hr>\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "current_script_dir = Path(__file__).parent\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Setup Parameters\n",
    "#\n",
    "#\n",
    "# Here we will load the configurations from _config.json_ file to\n",
    "# setup deployment_name, openai_api_base, openai_api_key and openai_api_version.\n",
    "\n",
    "# %%\n",
    "# Load config values\n",
    "with Path(current_script_dir / \"config.json\").open() as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# Setting up the deployment name\n",
    "deployment_name = config_details[\"GPT-4V_DEPLOYMENT_NAME\"]\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base = config_details[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai_api_key = #os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version = config_details[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Funciontion to Call GPT-4 Turbo with Vision API with Image\n",
    "\n",
    "\n",
    "# %%\n",
    "# Define GPT-4 Turbo with Vision API call with image\n",
    "def call_GPT4V_image(\n",
    "    messages: object,\n",
    "    ocr: bool = False,\n",
    "    grounding: bool = False,\n",
    "    face: bool = False,\n",
    "    in_context: object = None,\n",
    "    vision_api: object = None,\n",
    ") -> object:\n",
    "    # Construct the API request URL\n",
    "    if ocr or grounding or in_context is not None:\n",
    "        api_url = (\n",
    "            f\"{openai_api_base}/openai/deployments/{deployment_name}\"\n",
    "            f\"/extensions/chat/completions?api-version={openai_api_version}\"\n",
    "        )\n",
    "    else:\n",
    "        api_url = (\n",
    "            f\"{openai_api_base}/openai/deployments/{deployment_name}/chat/completions?api-version={openai_api_version}\"\n",
    "        )\n",
    "\n",
    "    # Including the api-key in HTTP headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": openai_api_key,\n",
    "        \"x-ms-useragent\": \"Azure-GPT-4V-image/1.0.0\",\n",
    "    }\n",
    "\n",
    "    if face:\n",
    "        headers[\"x-ms-useragent\"] = \"Azure-GPT-4V-image-face/1.0.0\"\n",
    "\n",
    "    # Payload for the request\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800,\n",
    "    }\n",
    "\n",
    "    if ocr or grounding:\n",
    "        payload[\"enhancements\"] = {\n",
    "            \"ocr\": {\"enabled\": ocr},  # Enable OCR enhancement\n",
    "            \"grounding\": {\"enabled\": grounding},  # Enable grounding enhancement\n",
    "        }\n",
    "\n",
    "    data_sources = []\n",
    "\n",
    "    if in_context is not None:\n",
    "        data_sources.append(\n",
    "            {\n",
    "                \"type\": \"AzureCognitiveSearch\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": in_context.get(\"endpoint\"),\n",
    "                    \"key\": in_context.get(\"key\"),\n",
    "                    \"indexName\": in_context.get(\"indexName\"),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if vision_api is not None:\n",
    "        data_sources.append(\n",
    "            {\n",
    "                \"type\": \"AzureComputerVision\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": vision_api.get(\"endpoint\"),\n",
    "                    \"key\": vision_api.get(\"key\"),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if data_sources:\n",
    "        payload[\"dataSources\"] = data_sources\n",
    "\n",
    "    # Send the request and handle the response\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Funciontion to Call GPT-4 Turbo with Vision API with Video Index\n",
    "\n",
    "\n",
    "# %%\n",
    "# Define GPT-4 Turbo with Vision API call with video index\n",
    "def call_GPT4V_video(messages: str, vision_api: object, video_index: object) -> object:\n",
    "    # Construct the API request URL\n",
    "    api_url = (\n",
    "        f\"{openai_api_base}/openai/deployments/{deployment_name}\"\n",
    "        f\"/extensions/chat/completions?api-version={openai_api_version}\"\n",
    "    )\n",
    "\n",
    "    # Including the api-key in HTTP headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": openai_api_key,\n",
    "        \"x-ms-useragent\": \"Azure-GPT-4V-video/1.0.0\",\n",
    "    }\n",
    "\n",
    "    # Payload for the request\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"dataSources\": [\n",
    "            {\n",
    "                \"type\": \"AzureComputerVisionVideoIndex\",\n",
    "                \"parameters\": {\n",
    "                    \"computerVisionBaseUrl\": f\"{vision_api.get('endpoint')}/computervision\",\n",
    "                    \"computerVisionApiKey\": vision_api.get(\"key\"),\n",
    "                    \"indexName\": video_index.get(\"video_index_name\"),\n",
    "                    \"videoUrls\": [video_index.get(\"video_SAS_url\")],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"enhancements\": {\"video\": {\"enabled\": True}},\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800,\n",
    "    }\n",
    "\n",
    "    # Send the request and handle the response\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Function to Create Video Index\n",
    "\n",
    "\n",
    "# %%\n",
    "def create_video_index(vision_api_endpoint: str, vision_api_key: str, index_name: str) -> object:\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\"features\": [{\"name\": \"vision\", \"domain\": \"surveillance\"}, {\"name\": \"speech\"}]}\n",
    "    return requests.put(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "\n",
    "def add_video_to_index(\n",
    "    vision_api_endpoint: str, vision_api_key: str, index_name: str, video_url: str, video_id: str\n",
    ") -> object:\n",
    "    url = (\n",
    "        f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}\"\n",
    "        f\"/ingestions/my-ingestion?api-version=2023-05-01-preview\"\n",
    "    )\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"videos\": [{\"mode\": \"add\", \"documentId\": video_id, \"documentUrl\": video_url}],\n",
    "        \"generateInsightIntervals\": False,\n",
    "        \"moderation\": False,\n",
    "        \"filterDefectedFrames\": False,\n",
    "        \"includeSpeechTranscrpt\": True,\n",
    "    }\n",
    "    return requests.put(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "\n",
    "def wait_for_ingestion_completion(\n",
    "    vision_api_endpoint: str, vision_api_key: str, index_name: str, max_retries: int = 30\n",
    ") -> bool:\n",
    "    url = (\n",
    "        f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions?api-version=2023-05-01-preview\"\n",
    "    )\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        time.sleep(10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            state_data = response.json()\n",
    "            if state_data[\"value\"][0][\"state\"] == \"Completed\":\n",
    "                print(state_data)\n",
    "                print(\"Ingestion completed.\")\n",
    "                return True\n",
    "            if state_data[\"value\"][0][\"state\"] == \"Failed\":\n",
    "                print(state_data)\n",
    "                print(\"Ingestion failed.\")\n",
    "                return False\n",
    "        retries += 1\n",
    "    return False\n",
    "\n",
    "\n",
    "def process_video_indexing(\n",
    "    vision_api_endpoint: str, vision_api_key: str, video_index_name: str, video_SAS_url: str, video_id: str\n",
    ") -> None:\n",
    "    # Step 1: Create an Index\n",
    "    response = create_video_index(vision_api_endpoint, vision_api_key, video_index_name)\n",
    "    print(response.status_code, response.text)\n",
    "\n",
    "    # Step 2: Add a video file to the index\n",
    "    response = add_video_to_index(vision_api_endpoint, vision_api_key, video_index_name, video_SAS_url, video_id)\n",
    "    print(response.status_code, response.text)\n",
    "\n",
    "    # Step 3: Wait for ingestion to complete\n",
    "    if not wait_for_ingestion_completion(vision_api_endpoint, vision_api_key, video_index_name):\n",
    "        print(\"Ingestion did not complete within the expected time.\")\n",
    "\n",
    "\n",
    "def call_face_API(image_file_path: str, face_api_endpoint: str, face_api_key: str) -> object:\n",
    "    \"\"\"\n",
    "    Calls a face recognition API and returns attributes of faces detected in the image.\n",
    "\n",
    "    Args:\n",
    "    image_file_path (str): Path to the image file.\n",
    "    face_api_endpoint (str): Endpoint URL of the face API.\n",
    "    face_api_key (str): Subscription key for the face API.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two objects with face attributes from different API parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def make_api_request(image: bytes, params: dict) -> dict:\n",
    "        \"\"\"Helper function to make API request and return response.\"\"\"\n",
    "        try:\n",
    "            face_api_url = face_api_endpoint + \"/face/v1.0/detect\"\n",
    "            response = requests.post(face_api_url, params=params, headers=headers, data=image)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to make the request. Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/octet-stream\",\n",
    "        \"Ocp-Apim-Subscription-Key\": face_api_key,\n",
    "        \"x-ms-useragent\": \"Azure-GPT-4V-image-face/1.0.0\",\n",
    "    }\n",
    "\n",
    "    params_01 = {\n",
    "        \"returnFaceId\": \"false\",\n",
    "        \"returnFaceLandmarks\": \"false\",\n",
    "        \"returnFaceAttributes\": \",\".join(\n",
    "            [\n",
    "                \"glasses\",\n",
    "                \"occlusion\",\n",
    "                \"accessories\",\n",
    "                \"blur\",\n",
    "                \"exposure\",\n",
    "                \"noise\",\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "    params_03 = {\n",
    "        \"returnFaceId\": \"false\",\n",
    "        \"returnFaceLandmarks\": \"false\",\n",
    "        \"detectionModel\": \"detection_03\",\n",
    "        \"recognitionModel\": \"recognition_04\",\n",
    "        \"returnFaceAttributes\": \",\".join([\"mask\", \"headPose\", \"qualityForRecognition\"]),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with Path(image_file_path).open(\"rb\") as image_file:\n",
    "            image = image_file.read()\n",
    "\n",
    "        attributes_01 = make_api_request(image, params_01)\n",
    "        attributes_03 = make_api_request(image, params_03)\n",
    "\n",
    "        return attributes_01, attributes_03\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\": deployment_name,\n",
    "    \"OPENAI_API_BASE\": openai_api_base,\n",
    "    \"OPENAI_API_VERSION\": openai_api_version,\n",
    "}\n",
    "\n",
    "p = Path(\"../config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shared_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m parent_dir \u001b[38;5;241m=\u001b[39m Path(Path\u001b[38;5;241m.\u001b[39mcwd())\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(parent_dir))\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshared_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call_GPT4V_image\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Image Tagging Assistant\u001b[39;00m\n\u001b[0;32m     10\u001b[0m image_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageTaggingAssistant.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shared_functions'"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from shared_functions import call_GPT4V_image\n",
    "\n",
    "# Image Tagging Assistant\n",
    "image_file_path = \"ImageTaggingAssistant.jpg\"\n",
    "sys_message = \"Generate a list of descriptive tags for the following image. Analyze the image carefully and produce tags that accurately represent the image. Ensure the tags are relevant.\"\n",
    "user_prompt = \"Provide tags for this image.\"\n",
    "\n",
    "# Encode the image in base64\n",
    "with Path(image_file_path).open(\"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"}},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    response_content = call_GPT4V_image(messages)\n",
    "    display(Image(image_file_path))\n",
    "    print(response_content[\"choices\"][0][\"message\"][\"content\"])  # Print the content of the response\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4 Turbo with Vision API. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
